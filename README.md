# titanic_gpt2_fine_tuned
kaggle titanic tutorial by gpt2 fine-tuning
=====================================
#Titanic
https://www.kaggle.com/competitions/titanic

#Approach




#HyperParameter



#Curious Things
1.pad_token  = eos_token makes sense?
2.special token length matter?  (1 or more)


#Result
ðŸ¤®Score: 0.67464ðŸ¤®

#Why it sucks?
- maybe no sufficient training data
- maybe inappropriate model for task
- maybe wrong hyperparameters

#Reference

1. GPT2 fine tuning for classification
https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb

2. Simple Chit-Chat based on KoGPT2
https://github.com/haven-jeon/KoGPT2-chatbot
